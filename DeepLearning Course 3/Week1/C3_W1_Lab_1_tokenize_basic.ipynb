{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEVjErO0twmuHsybJ/UHdV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitpal16181/deepmindAICoursera_codePractice/blob/main/C3_W1_Lab_1_tokenize_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ungraded Lab: Tokenizer Basics"
      ],
      "metadata": {
        "id": "6a-JYgzI7EYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In most NLP tasks, the initial step in preparing your data is to extract a vocabulary of words from your *corpus* (i.e. input texts). You will need to define how to represent the texts into numerical representations which can be used to train a neural network. These representations are called tokens and Tensorflow and Keras makes it easy to generate these using its APIs. You will see how to do that in the next cells."
      ],
      "metadata": {
        "id": "mUmfMRzX7IAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the vocabulary"
      ],
      "metadata": {
        "id": "_jH73pMN7Ops"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, you will look first at how you can provide a look up dictionary for each word. The code below takes a list of sentences, then takes each word in those sentences and assigns it to an integer. This is done using the  [fit_on_texts()](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts) method and you can get the result by looking at the `word_index` property. More frequent words have a lower index."
      ],
      "metadata": {
        "id": "y0EtAyGF7QLk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUdHE7NB67pA",
        "outputId": "0d761810-a881-4bf4-d06a-8731f1bc8ee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'i': 1, 'love': 2, 'my': 3, 'dog': 4, 'cat': 5}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "## Defining input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat'\n",
        "]\n",
        "\n",
        "# Initializing the Tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "\n",
        "# Generating indices for each word in the corous\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Getting the indices and printing it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `num_words` parameter used in the initializer specifies the maximum number of words minus one (based on frequency) to keep when generating sequences. You will see this in a later exercise. For now, the important thing to note is it does not affect how the `word_index` dictionary is generated. You can try passing `1` instead of `100` as shown on the next cell and you will arrive at the same `word_index`."
      ],
      "metadata": {
        "id": "n6o-KLly8uW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also notice that by default, all punctuation is ignored and words are converted to lower case. You can override these behaviors by modifying the 'filters' and 'lower' arguments of the 'Tokenizer' class as described [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#arguments). You can try modifying these in the next cell below and compare the output to the one generated above."
      ],
      "metadata": {
        "id": "kgUX6XNE89BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining input sentences\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "# Initialize the tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 1)\n",
        "\n",
        "# Generating indices for each word in the corpus\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Getting the indices and print it\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psPoeb039P6F",
        "outputId": "83663385-dd58-40cb-c82d-ceaaee0f20e3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That concludes this short exercise on tokenizing input texts!"
      ],
      "metadata": {
        "id": "Qcu9c5Oy93Jb"
      }
    }
  ]
}