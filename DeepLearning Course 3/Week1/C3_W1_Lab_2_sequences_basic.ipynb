{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNxUAXuva4jopqlAubUU+m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitpal16181/deepmindAICoursera_codePractice/blob/main/C3_W1_Lab_2_sequences_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ungraded Lab: Generating Sequences and Padding"
      ],
      "metadata": {
        "id": "qjsS1CoxAHFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, you will look at converting your input sentences into a sequence of tokens. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections.\n",
        "\n"
      ],
      "metadata": {
        "id": "PinExO3aAKZi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text to Sequences"
      ],
      "metadata": {
        "id": "uriz6WwzAM_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous lab, you saw how to generate a `word_index` dictionary to generate tokens for each word in your corpus. You can use then use the result to convert each of the input sentences into a sequence of tokens. That is done using the `texts_to_sequences()` method as shown below."
      ],
      "metadata": {
        "id": "ch1GlZTEAPHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBO-2d8uAEbR",
        "outputId": "a4aef264-5b2d-400a-8f71-2cbccc9b68a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Sequences =  [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Defining input texts\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "# Initializing the tokenizer class\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>')\n",
        "\n",
        "# Tokenizing the input sentences\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "\n",
        "# Getting the word index dictionary\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Generating list of token sequences\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "\n",
        "# Printing the result\n",
        "print('\\nWord index = ', word_index)\n",
        "print('\\nSequences = ', sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding\n",
        "As mentioned in the lecture, you will usually need to pad the sequences into a uniform length because that is what your model expects. You can use the [pad_sequences](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) for that. By default, it will pad according to the length of the longest sequence. You can override this with the `maxlen` argument to define a specific length. Feel free to play with the [other arguments](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences#args) shown in class and compare the result."
      ],
      "metadata": {
        "id": "6DOrHePaF8Pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding the sequences to a uniform length\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "\n",
        "# Printing the result\n",
        "print('\\nPadded sequences:')\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIUjXdIyGSC3",
        "outputId": "5b435793-b963-4a1a-9395-a6a1145b5a44"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Padded sequences:\n",
            "[[ 0  5  3  2  4]\n",
            " [ 0  5  3  2  7]\n",
            " [ 0  6  3  2  4]\n",
            " [ 9  2  4 10 11]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Out-of-vocabulary tokens"
      ],
      "metadata": {
        "id": "je75zNSRLuE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that you defined an `oov_token` when the `Tokenizer` was initialized earlier. This will be used when you have input words that are not found in the `word_index` dictionary. For example, you may decide to collect more text after your initial training and decide to not re-generate the `word_index`. You will see this in action in the cell below. Notice that the token `1` is inserted for words that are not found in the dictionary."
      ],
      "metadata": {
        "id": "QrsYbgkULwW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying with words that the tokenizer wasn't fit to\n",
        "test_data = [\n",
        "    'I really love my dog',\n",
        "    'my dog loves my manatee'\n",
        "]\n",
        "\n",
        "# Generating the swquence\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "\n",
        "# Printing the word index dictionary\n",
        "print('\\nWord Index = ', word_index)\n",
        "\n",
        "# Printing the sequences with OOV\n",
        "print('\\nTest Sequence = ', test_seq)\n",
        "\n",
        "# Printing the padded result\n",
        "padded = pad_sequences(test_seq, maxlen=10)\n",
        "print('\\nPaddes Test Sequence: ')\n",
        "print(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhFYkSQCL7f2",
        "outputId": "742b3dda-90bc-4aed-e5d9-c42af41c41b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
            "\n",
            "Test Sequence =  [[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n",
            "\n",
            "Paddes Test Sequence: \n",
            "[[0 0 0 0 0 5 1 3 2 4]\n",
            " [0 0 0 0 0 2 4 1 2 1]]\n"
          ]
        }
      ]
    }
  ]
}